'''Objective:- Is to create a Machine Learning Pipeline that will generate a pkl file. 
The web application depends on the .pkl file generated by the ML pipeline.

 To run ML pipeline that trains classifier and saves, run following in the project's root directory:-
  `python models/train_classifier.py data/DisasterResponse.db models/classifier.pkl`
'''

#1.Import Libraries
import sys
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
from sklearn.base import BaseEstimator,TransformerMixin
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer
from sklearn.multioutput import MultiOutputClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier, AdaBoostClassifier
import pickle
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('averaged_perceptron_tagger')
from sklearn.metrics import classification_report, accuracy_score


#2. Read data from Sqlite Database
def load_data(database_filepath):
    
    '''Load Data from sqlite using database_filepath, this path will take you to DisasterResponse.db.
     This function will produce following:1. X axis with features 2) Y axis with Targets 3)List of categories.
    
    Note:- Always use function, never hard code the database.
    Guided by Udacity mentor. I hardcoded it which created issue on executing run.py.'''
   
    engine = create_engine('sqlite:///'+database_filepath)
    df = pd.read_sql_table('DisasterResponse', engine)
    X = df['message']
    y = df.iloc[:,4:]
    category_names = y.columns
    return X,y, category_names


##Define url_regex to search the entire URL for the regular expression you specify
url_regex = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\(\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'


#3 Split the text using tokenize function
def tokenize(text):
    
    '''Normalize the text by removing upper case,punctuations and special characters'''
    
    detected_urls = re.findall(url_regex, text)
    for url in detected_urls:
        text = text.replace(url, "urlplaceholder")

 #This will tokenize words    
    tokens = nltk.word_tokenize(text)
    lemmatizer =nltk. WordNetLemmatizer()

 ##Produce clean tokens
    clean_tokens = []
    for tok in tokens:
        clean_tok = lemmatizer.lemmatize(tok).lower().strip() 
        clean_tokens.append(clean_tok)

    return clean_tokens


class StartingVerbExtractor(BaseEstimator, TransformerMixin):

    def starting_verb(self, text):
        
        '''In this code block we are converting a sentence to forms â€“ list of words.The tag in this case is  part-of-speech tag.'''
        
        sentence_list = nltk.sent_tokenize(text)
        for sentence in sentence_list:
            pos_tags = nltk.pos_tag(tokenize(sentence))
            first_word, first_tag = pos_tags[0]
            if first_tag in ['VB', 'VBP'] or first_word == 'RT':
                return True
        return False

        
#As it is a transformer, hence returning self.    
    def fit(self, x, y=None):
        return self

          
    def transform(self, X):
        X_tagged = pd.Series(X).apply(self.starting_verb)
        return pd.DataFrame(X_tagged)
    
      
#4 Build the pipeline    
def build_model():
    
    '''Built a Machine Learning Pipeline with AdaBoostClassifier, first built with Randomforest classifer but it was super slow.
    Based on suggestions in peer help forum shifted to AdaBoost classifier and it worked fine' and produced GridSearch Output'''
    
    pipeline = Pipeline([
        ('features', FeatureUnion([

            ('text_pipeline', Pipeline([
                ('vect', CountVectorizer(tokenizer=tokenize)),
                ('tfidf', TfidfTransformer())
            ])),

            ('starting_verb', StartingVerbExtractor())
        ])),

        ('clf', MultiOutputClassifier(AdaBoostClassifier()))
    ])

    parameters_grid = {'clf__estimator__n_estimators': [10, 20]}
    pipeline_cv = GridSearchCV(pipeline, param_grid=parameters_grid, scoring='f1_micro', n_jobs=-1,cv=2,verbose=3)

    return pipeline_cv


#5.Evaluate your model
def evaluate_model(model, X_test, Y_test, category_names):
    '''validate the model performance'''
    
    y_pred = model.predict(X_test)
    print(classification_report(Y_test, y_pred, target_names=category_names))
    results = pd.DataFrame(columns=['Category', 'f_score', 'precision', 'recall'])

#6.Save your model        
def save_model(model, model_filepath):
    '''This function saves trained model as Pickle file.'''
    pickle.dump(model, open(model_filepath, 'wb'))


    
#7. Execute your program    
def main():
    if len(sys.argv) == 3:
        database_filepath, model_filepath = sys.argv[1:]
        print('Loading data...\n    DATABASE: {}'.format(database_filepath))
        X, Y, category_names = load_data(database_filepath)
        X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)
        
        print('Building model...')
        model = build_model()
        
        print('Training model...')
        model.fit(X_train, Y_train)
        
        print('Evaluating model...')
        evaluate_model(model, X_test, Y_test, category_names)

        print('Saving model...\n    MODEL: {}'.format(model_filepath))
        save_model(model, model_filepath)

        print('Trained model saved!')

    else:
        print('Please provide the filepath of the disaster messages database '\
              'as the first argument and the filepath of the pickle file to '\
              'save the model to as the second argument. \n\nExample: python '\
              'train_classifier.py ../data/DisasterResponse.db classifier.pkl')


if __name__ == '__main__':
    main()